
<!DOCTYPE html>

<html lang="en">
<head>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta charset="utf-8"/>
<title>ddql_optimal_execution.agent._ddql | DDQL Optimal Execution  documentation</title>
<link href="../../../_static/pygments.css" rel="stylesheet"/>
<link href="../../../_static/theme.2e8060a090e1247124e3.css" rel="stylesheet"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="../../../genindex.html" rel="index" title="Index"/>
</head>
<body class="antialiased text-gray scroll-smooth">
<div class="min-h-screen xl:h-screen flex flex-col xl:grid xl:grid-layout print:block print:h-auto" data-action="keydown@window-&gt;search#focus " data-controller="sidebar search " id="page">
<a class="block transition -translate-x-full focus:translate-x-0 opacity-0 focus:opacity-100 text-xl bg-white p-4 z-20 absolute top-0 left-0 h-14" href="#ddql-optimal-execution-agent-ddql" title="Skip navigation links">Skip to content</a>
<header class="grid-area-header z-10 h-14 fixed w-full top-0 print:hidden">
<div class="bg-gray-dark shadow-md flex items-center h-full xl:px-2 relative"><div class="flex items-center">
<button class="xl:hidden h-14 w-14 leading-14 text-gray-100 hover:bg-gray-700 hover:text-brand focus:outline-none focus:bg-gray-700 focus:text-brand" data-action="sidebar#open" data-sidebar-target="hamburger">
<span class="sr-only">Open navigation menu</span>
<svg aria-hidden="true" class="fill-current h-8 w-8" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"></path></svg>
</button><a class="hover:bg-gray-700 focus:bg-gray-700 focus:outline-none" href="../../../index.html" title="Back to homepage"><span class="hidden lg:inline-block shrink-0 font-medium text-gray-100 mx-5 leading-14 tracking-wider">DDQL Optimal Execution  documentation</span>
</a></div><div class="flex justify-end items-center flex-1"><form action="../../../search.html" class="flex print:hidden justify-between items-center leading-14 md:ml-4 bg-gray-dark text-gray-300 focus-within:bg-gray-50 focus-within:text-gray-800 focus-within:absolute focus-within:inset-x-0 focus-within:top-0 md:focus-within:w-full md:focus-within:static z-10" data-action="click-&gt;search#focusSearchInput" id="searchbox" method="get">
<button aria-label="Get search results" class="text-inherit h-14 w-14" tabindex="-1">
<svg aria-hidden="true" class="fill-current stroke-current h-8 w-8" stroke-width="0.5" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M15.5 14h-.79l-.28-.27A6.471 6.471 0 0016 9.5 6.5 6.5 0 109.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"></path></svg>
</button>
<input aria-label="Search the docs" class="pr-2 bg-transparent text-inherit focus:outline-none w-0 md:w-auto focus:w-full transition-all duration-100" data-search-target="searchInput" id="search-input" name="q" placeholder="Search the docs" type="search"/>
</form></div>
</div>
</header>
<aside class="grid-area-sidebar h-full fixed pt-14 xl:relative inset-y-0 left-0 z-20 xl:z-0 print:hidden overflow-y-auto transition-all transform transform-gpu -translate-x-full opacity-0 duration-300 xl:translate-x-0 xl:opacity-100" data-sidebar-target="sidebar">
<nav class="h-full overflow-y-auto bg-white text-gray-600 pt-8 flex flex-col" role="navigation">
<div class="nav-toc flex-1 pl-6"><p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><div class="nav-link"><a class="reference internal" data-action="click-&gt;sidebar#close" href="../../../usage.html">Installation</a></div></li>
<li class="toctree-l1"><div class="nav-link"><a class="reference internal" data-action="click-&gt;sidebar#close" href="../../../quickstart.html">Quickstart Guide</a></div></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Main Objects:</span></p>
<ul>
<li class="toctree-l1"><div class="nav-link"><a class="reference internal" data-action="click-&gt;sidebar#close" href="../../../stubs/ddql_optimal_execution.preprocessing.Preprocessor.html">ddql_optimal_execution.preprocessing.Preprocessor</a></div></li>
<li class="toctree-l1"><div class="nav-link"><a class="reference internal" data-action="click-&gt;sidebar#close" href="../../../stubs/ddql_optimal_execution.experience_replay.ExperienceReplay.html">ddql_optimal_execution.experience_replay.ExperienceReplay</a></div></li>
<li class="toctree-l1"><div class="nav-link"><a class="reference internal" data-action="click-&gt;sidebar#close" href="../../../stubs/ddql_optimal_execution.state.State.html">ddql_optimal_execution.state.State</a></div></li>
<li class="toctree-l1"><div class="nav-link"><a class="reference internal" data-action="click-&gt;sidebar#close" href="../../../stubs/ddql_optimal_execution.state.StateArray.html">ddql_optimal_execution.state.StateArray</a></div></li>
<li class="toctree-l1"><div class="nav-link"><a class="reference internal" data-action="click-&gt;sidebar#close" href="../../../stubs/ddql_optimal_execution.agent.DDQL.html">ddql_optimal_execution.agent.DDQL</a></div></li>
<li class="toctree-l1"><div class="nav-link"><a class="reference internal" data-action="click-&gt;sidebar#close" href="../../../stubs/ddql_optimal_execution.agent.TWAP.html">ddql_optimal_execution.agent.TWAP</a></div></li>
<li class="toctree-l1"><div class="nav-link"><a class="reference internal" data-action="click-&gt;sidebar#close" href="../../../stubs/ddql_optimal_execution.environnement.MarketEnvironnement.html">ddql_optimal_execution.environnement.MarketEnvironnement</a></div></li>
</ul>
</div>
<button class="text-4xl text-gray-800 p-4 bottom-0 hover:text-brand xl:hidden focus:text-brand self-center" data-action="sidebar#close" title="Close navigation menu">
<span class="sr-only">Close navigation menu</span>
<svg aria-hidden="true" class="fill-current stroke-current h-6 w-6" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
</button>
</nav>
</aside>
<main class="px-4 pt-14 xl:ml-fluid grid-area-main overflow-y-auto flex flex-col flex-1 h-full mx-0 md:mx-auto xl:mr-0"><nav aria-label="breadcrumbs" class="print:hidden mt-12 text-sm text-gray-light" role="navigation">
<a class="text-gray-light text-sm hover:text-gray-dark font-medium focus:text-gray-dark" href="../../../index.html">DDQL Optimal Execution  documentation</a>
<span class="mr-1">/</span><a class="text-gray-light text-sm hover:text-gray-dark" href="../../index.html">Module code</a>
<span class="mr-1">/</span><span aria-current="page">ddql_optimal_execution.agent._ddql</span>
</nav>
<article class="flex-1" role="main">
<h1>Source code for ddql_optimal_execution.agent._ddql</h1><div class="highlight"><pre>
<code><span class="kn">from</span> <span class="nn">._agent</span> <span class="kn">import</span> <span class="n">Agent</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>

<span class="kn">from</span> <span class="nn">._neural_net</span> <span class="kn">import</span> <span class="n">QNet</span>
<span class="kn">from</span> <span class="nn">ddql_optimal_execution</span> <span class="kn">import</span> <span class="n">State</span><span class="p">,</span> <span class="n">get_device</span>


<div class="viewcode-block" id="DDQL"><a class="viewcode-back" href="../../../stubs/ddql_optimal_execution.agent.DDQL.html#ddql_optimal_execution.agent.DDQL">[docs]</a><span class="k">class</span> <span class="nc">DDQL</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    The DDQL class inherits from the Agent class. It is an agent that implements a Double Deep Q-Learning algorithm.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    state_dict : dict, optional</span>
<span class="sd">        A dictionary containing the state of the agent, by default None</span>
<span class="sd">    greedy_decay_rate : float, optional</span>
<span class="sd">        The greedy decay rate, by default 0.95</span>
<span class="sd">    target_update_rate : int, optional</span>
<span class="sd">        The target update rate, by default 15</span>
<span class="sd">    initial_greediness : float, optional</span>
<span class="sd">        The initial greediness, by default 1</span>
<span class="sd">    mode : str, optional</span>
<span class="sd">        The mode, by default "train"</span>
<span class="sd">    lr : float, optional</span>
<span class="sd">        The learning rate, by default 1e-3</span>
<span class="sd">    state_size : int, optional</span>
<span class="sd">        The state size, by default 5</span>
<span class="sd">    initial_budget : int, optional</span>
<span class="sd">        The initial budget, by default 100</span>
<span class="sd">    horizon : int, optional</span>
<span class="sd">        The horizon, by default 100</span>
<span class="sd">    gamma : float, optional</span>
<span class="sd">        The gamma parameter used in the Q-Learning algorithm, by default 0.99</span>
<span class="sd">    quadratic_penalty_coefficient : float, optional</span>
<span class="sd">        The quadratic penalty coefficient used to penalize the agent for selling big quantities of stocks, by default 0.01</span>


<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    device : torch.device</span>
<span class="sd">        The device used to run the agent</span>
<span class="sd">    main_net : QNet</span>
<span class="sd">        The main neural network used to predict the Q-values of the state-action pairs</span>
<span class="sd">    target_net : QNet</span>
<span class="sd">        The target neural network used to predict the Q-values of the state-action pairs</span>
<span class="sd">    state_size : int</span>
<span class="sd">        The state size</span>
<span class="sd">    greedy_decay_rate : float</span>
<span class="sd">        The greedy decay rate</span>
<span class="sd">    target_update_rate : int</span>
<span class="sd">        The target update rate</span>
<span class="sd">    initial_greediness : float</span>
<span class="sd">        The initial greediness of the agent. It is used to determine the probability of the agent taking a random action.</span>
<span class="sd">    greediness : float</span>
<span class="sd">        The current greediness of the agent.</span>
<span class="sd">    mode : str</span>
<span class="sd">        The mode of the agent. It can be either "train" or "test".</span>
<span class="sd">    lr : float</span>
<span class="sd">        The learning rate used to update the weights of the neural network.</span>
<span class="sd">    gamma : float</span>
<span class="sd">        The gamma parameter used in the Q-Learning algorithm.</span>
<span class="sd">    quadratic_penalty_coefficient : float</span>
<span class="sd">        The quadratic penalty coefficient used to penalize the agent for selling big quantities of stocks.</span>
<span class="sd">    optimizer : torch.optim</span>
<span class="sd">        The optimizer used to update the weights of the neural network.</span>
<span class="sd">    loss_fn : torch.nn</span>
<span class="sd">        The loss function used to calculate the loss between the predicted Q-values and the target Q-values.</span>


<span class="sd">    """</span>

<div class="viewcode-block" id="DDQL.__init__"><a class="viewcode-back" href="../../../stubs/ddql_optimal_execution.agent.DDQL.html#ddql_optimal_execution.agent.DDQL.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">greedy_decay_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.95</span><span class="p">,</span>
        <span class="n">target_update_rate</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
        <span class="n">initial_greediness</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"train"</span><span class="p">,</span>
        <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">state_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">initial_budget</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">horizon</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span>
        <span class="n">quadratic_penalty_coefficient</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">verbose</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">initial_budget</span><span class="p">,</span> <span class="n">horizon</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_device</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using </span><span class="si">{self.device}</span><span class="s2"> device"</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">main_net</span> <span class="o">=</span> <span class="n">QNet</span><span class="p">(</span><span class="n">state_size</span><span class="o">=</span><span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="o">=</span><span class="n">initial_budget</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_net</span> <span class="o">=</span> <span class="n">QNet</span><span class="p">(</span><span class="n">state_size</span><span class="o">=</span><span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="o">=</span><span class="n">initial_budget</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span> <span class="o">=</span> <span class="n">state_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>

        <span class="k">if</span> <span class="n">state_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">main_net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target_net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">greedy_decay_rate</span> <span class="o">=</span> <span class="n">greedy_decay_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_update_rate</span> <span class="o">=</span> <span class="n">target_update_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">greediness</span> <span class="o">=</span> <span class="n">initial_greediness</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quadratic_penalty_coefficient</span> <span class="o">=</span> <span class="n">quadratic_penalty_coefficient</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">learning_step</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">"train"</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">main_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span></div>

<div class="viewcode-block" id="DDQL.train"><a class="viewcode-back" href="../../../stubs/ddql_optimal_execution.agent.DDQL.html#ddql_optimal_execution.agent.DDQL.train">[docs]</a>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">"""This function sets the mode to "train" and trains the main neural network."""</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">main_net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="s2">"train"</span></div>

<div class="viewcode-block" id="DDQL.eval"><a class="viewcode-back" href="../../../stubs/ddql_optimal_execution.agent.DDQL.html#ddql_optimal_execution.agent.DDQL.eval">[docs]</a>    <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">"""This function sets the mode to "eval" and puts the main network in evaluation mode."""</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">main_net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="s2">"eval"</span></div>

<div class="viewcode-block" id="DDQL.get_action"><a class="viewcode-back" href="../../../stubs/ddql_optimal_execution.agent.DDQL.html#ddql_optimal_execution.agent.DDQL.get_action">[docs]</a>    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">"""This function returns a tensor that is either a random binomial distribution or the index of the</span>
<span class="sd">        maximum value in the output of a neural network, depending on certain conditions.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        state : State</span>
<span class="sd">            The `state` parameter is an instance of the `State` class, which contains information about the</span>
<span class="sd">        current state of the environment in which the agent is operating. This information typically</span>
<span class="sd">        includes things like the agent's current position, the state of the game board, and any other</span>
<span class="sd">        relevant information that the agent needs</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">            an integer that represents the action to be taken based on the given state. If the `greediness`</span>
<span class="sd">        parameter is set and the `mode` is "train", a random binomial distribution is generated using the</span>
<span class="sd">        state's inventory as the number of trials and the probability of success as 1/inventory. Otherwise,</span>
<span class="sd">        the action is determined by the main neural network's output, which is the index of the maximum</span>
<span class="sd">        value in the output Q-values tensor.</span>

<span class="sd">        """</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"inventory"</span><span class="p">],</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">state</span><span class="p">[</span><span class="s2">"inventory"</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">greediness</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">"train"</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">main_net</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">__update_target_net</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">"""This function updates the target network by loading the state dictionary of the main network."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">main_net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">__complete_target</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">experience_batch</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="sd">"""This function takes in a batch of experiences and returns the corresponding targets, actions, and</span>
<span class="sd">        states for training a reinforcement learning agent.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        experience_batch : np.ndarray</span>
<span class="sd">            `experience_batch` is a numpy array containing a batch of experiences. Each experience is a</span>
<span class="sd">        dictionary containing information about a single step taken by the agent in the environment. The</span>
<span class="sd">        dictionary contains keys such as "state", "action", "reward", "next_state", and "dist2Horizon".</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">            a tuple of three torch Tensors: targets, actions, and states.</span>

<span class="sd">        """</span>
        <span class="n">targets</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">experience_batch</span><span class="p">)),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">experience_batch</span><span class="p">)),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">experience_batch</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">)),</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">experience</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">experience_batch</span><span class="p">):</span>  <span class="c1"># can be vectorized</span>
            <span class="n">actions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">experience</span><span class="p">[</span><span class="s2">"action"</span><span class="p">]</span>
            <span class="n">states</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">experience</span><span class="p">[</span><span class="s2">"state"</span><span class="p">]</span><span class="o">.</span><span class="n">astensor</span>
            <span class="k">if</span> <span class="n">experience</span><span class="p">[</span><span class="s2">"dist2Horizon"</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">experience</span><span class="p">[</span><span class="s2">"reward"</span><span class="p">]</span>

            <span class="k">elif</span> <span class="n">experience</span><span class="p">[</span><span class="s2">"dist2Horizon"</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">experience</span><span class="p">[</span><span class="s2">"reward"</span><span class="p">]</span>
                    <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span>
                    <span class="o">*</span> <span class="n">experience</span><span class="p">[</span><span class="s2">"next_state"</span><span class="p">][</span><span class="s2">"inventory"</span><span class="p">](</span>
                        <span class="n">experience</span><span class="p">[</span><span class="s2">"next_state"</span><span class="p">][</span><span class="s2">"Price"</span><span class="p">]</span> <span class="o">-</span> <span class="n">experience</span><span class="p">[</span><span class="s2">"state"</span><span class="p">][</span><span class="s2">"Price"</span><span class="p">]</span>
                    <span class="p">)</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">quadratic_penalty_coefficient</span>
                    <span class="o">*</span> <span class="p">(</span><span class="n">experience</span><span class="p">[</span><span class="s2">"next_state"</span><span class="p">][</span><span class="s2">"inventory"</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">best_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">main_net</span><span class="p">(</span><span class="n">experience</span><span class="p">[</span><span class="s2">"next_state"</span><span class="p">])</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">experience</span><span class="p">[</span><span class="s2">"reward"</span><span class="p">]</span>
                    <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span>
                    <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_net</span><span class="p">(</span><span class="n">experience</span><span class="p">[</span><span class="s2">"next_state"</span><span class="p">])[</span><span class="nb">int</span><span class="p">(</span><span class="n">best_action</span><span class="p">)]</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">targets</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">states</span>

<div class="viewcode-block" id="DDQL.learn"><a class="viewcode-back" href="../../../stubs/ddql_optimal_execution.agent.DDQL.html#ddql_optimal_execution.agent.DDQL.learn">[docs]</a>    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experience_batch</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">"""This function trains a neural network using a batch of experiences and updates the target network</span>
<span class="sd">        periodically.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        experience_batch : np.ndarray</span>
<span class="sd">            The experience_batch parameter is a numpy array containing a batch of experiences, where each</span>
<span class="sd">        experience is a tuple of (state, action, reward, next_state, dist2Horizon). This batch is used to update the</span>
<span class="sd">        neural network's weights through backpropagation.</span>

<span class="sd">        """</span>

        <span class="n">targets</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__complete_target</span><span class="p">(</span><span class="n">experience_batch</span><span class="p">)</span>
        <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">TensorDataset</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">targets</span><span class="p">),</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">main_net</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">learning_step</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">greediness</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">greediness</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">greedy_decay_rate</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_step</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_update_rate</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="o">==</span><span class="kc">True</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__update_target_net</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"Target network updated at step </span><span class="si">{self.learning_step}</span><span class="s2"> with greediness </span><span class="si">{self.greediness:.2f}</span><span class="s2">"</span>
            <span class="p">)</span></div></div>
</code></pre></div>
</article>
<footer>
<div class="mt-20 mb-4 text-sm text-gray-700 print:mt-4">© 2023, g0bel1n Made with <a href="https://www.sphinx-doc.org">Sphinx 4.5.0</a></div>
</footer></main>
<button class="fixed bottom-0 right-0 z-20 opacity-0 p-4 m-4 tracking-wide bg-gray-900 text-gray-100 transition transform transform-gpu duration-500 translate-y-full" data-action="search#hideSnackbar" data-search-target="snackbar">
  Clear highlights
</button><div class="fixed hidden inset-0 bg-black bg-opacity-50" data-action="click-&gt;sidebar#close" data-sidebar-target="screen">
</div>
</div>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/theme.b62e1ded0c8c099a2d47.js"></script>
</body>
</html>