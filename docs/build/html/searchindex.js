Search.setIndex({"docnames": ["index", "quickstart", "stubs/ddql_optimal_execution.agent.DDQL", "stubs/ddql_optimal_execution.agent.TWAP", "stubs/ddql_optimal_execution.environnement.MarketEnvironnement", "stubs/ddql_optimal_execution.experience_replay.ExperienceReplay", "stubs/ddql_optimal_execution.preprocessing.Preprocessor", "stubs/ddql_optimal_execution.state.State", "stubs/ddql_optimal_execution.state.StateArray", "stubs/ddql_optimal_execution.trainer.Trainer", "usage"], "filenames": ["index.rst", "quickstart.rst", "stubs/ddql_optimal_execution.agent.DDQL.rst", "stubs/ddql_optimal_execution.agent.TWAP.rst", "stubs/ddql_optimal_execution.environnement.MarketEnvironnement.rst", "stubs/ddql_optimal_execution.experience_replay.ExperienceReplay.rst", "stubs/ddql_optimal_execution.preprocessing.Preprocessor.rst", "stubs/ddql_optimal_execution.state.State.rst", "stubs/ddql_optimal_execution.state.StateArray.rst", "stubs/ddql_optimal_execution.trainer.Trainer.rst", "usage.rst"], "titles": ["Welcome to DDQL Optimal Execution\u2019s documentation!", "Quickstart Guide", "ddql_optimal_execution.agent.DDQL", "ddql_optimal_execution.agent.TWAP", "ddql_optimal_execution.environnement.MarketEnvironnement", "ddql_optimal_execution.experience_replay.ExperienceReplay", "ddql_optimal_execution.preprocessing.Preprocessor", "ddql_optimal_execution.state.State", "ddql_optimal_execution.state.StateArray", "ddql_optimal_execution.trainer.Trainer", "Installation"], "terms": {"instal": 0, "quickstart": 0, "guid": 0, "from": [1, 2, 4, 5, 7, 9], "ddql_optimal_execut": 1, "import": 1, "ddql": [1, 9, 10], "marketenvironn": [1, 9], "experiencereplai": [1, 9], "twap": 1, "creat": [1, 7], "environn": 1, "env": [1, 9], "initial_inventori": [1, 4], "500": 1, "multi_episod": [1, 4], "true": [1, 4, 6], "agent": [1, 4, 5, 6, 9], "state_s": [1, 2], "initial_budget": [1, 2, 3], "horizon": [1, 2, 3, 4, 5], "experi": [1, 2, 5, 9], "replai": [1, 5, 9], "store": [1, 5, 9], "exp_replai": [1, 9], "capac": [1, 5, 9], "1000": [1, 9], "train": [1, 2, 5, 9], "everi": 1, "detect": 1, "episod": [1, 4, 5, 9], "rang": 1, "len": 1, "historical_data_seri": [1, 4], "swap_episod": [1, 4], "while": 1, "done": [1, 2, 4, 5], "current_st": 1, "get_stat": [1, 4], "action": [1, 2, 3, 4, 5, 6, 9], "reward": [1, 2, 4, 5], "step": [1, 2, 3, 4, 5, 9], "distance2horizon": 1, "state": [1, 2, 3, 4, 5, 9], "period": [1, 2, 4, 6], "push": [1, 5], "copi": [1, 4, 7], "1": [1, 2, 6], "els": 1, "2": 1, "learn": [1, 2, 5, 9], "sampl": [1, 5, 9], "128": [1, 5], "class": [2, 3, 4, 5, 6, 7, 8, 9], "state_dict": 2, "dict": 2, "none": [2, 4, 5, 6, 8], "greedy_decay_r": 2, "float": [2, 4, 5, 7], "0": [2, 4, 6], "95": 2, "target_update_r": 2, "int": [2, 3, 4, 5, 6, 9], "15": 2, "initial_greedi": 2, "mode": [2, 4], "str": [2, 4], "lr": 2, "001": 2, "5": [2, 4], "100": [2, 3, 4], "gamma": 2, "99": 2, "quadratic_penalty_coeffici": [2, 4], "01": [2, 4], "sourc": [2, 3, 4, 5, 6, 7, 8, 9, 10], "The": [2, 3, 4, 5, 7, 9, 10], "inherit": 2, "It": [2, 3, 4, 5, 6, 9], "i": [2, 3, 4, 5, 6, 7, 9, 10], "an": [2, 3, 4, 5, 6, 7, 9], "implement": 2, "doubl": 2, "deep": 2, "q": [2, 5], "algorithm": [2, 5, 9], "paramet": [2, 3, 4, 5, 6, 9], "option": [2, 4, 5, 6, 9], "A": [2, 4, 6], "dictionari": [2, 4, 7], "contain": [2, 3, 4], "default": [2, 4], "greedi": 2, "decai": 2, "rate": 2, "target": 2, "updat": [2, 4, 5, 7], "initi": [2, 4, 6, 9], "1e": 2, "3": 2, "size": [2, 5], "budget": [2, 3], "us": [2, 4, 5, 6, 7, 9], "quadrat": [2, 4, 6], "penalti": [2, 4], "coeffici": [2, 4], "penal": 2, "sell": [2, 4, 9], "big": 2, "quantiti": [2, 4], "stock": 2, "devic": 2, "run": [2, 9], "type": [2, 4, 5, 6, 7, 9], "torch": [2, 7], "main_net": 2, "main": 2, "neural": 2, "network": 2, "predict": 2, "valu": [2, 3, 4, 5, 6, 7, 9], "pair": [2, 5, 7], "qnet": 2, "target_net": 2, "determin": [2, 4, 5, 6], "probabl": 2, "take": [2, 3, 5, 6, 9], "random": [2, 5, 9], "current": [2, 3, 4, 5, 7, 9], "can": [2, 5, 6, 10], "either": 2, "test": [2, 9], "weight": 2, "optim": [2, 10], "loss_fn": 2, "loss": 2, "function": [2, 3, 4, 5, 6, 7, 9], "calcul": [2, 4, 6], "between": 2, "nn": 2, "__init__": [2, 3, 4, 5, 6, 7, 8, 9], "method": [2, 3, 4, 5, 6, 7, 8, 9], "__complete_target": 2, "experience_batch": 2, "ndarrai": 2, "tupl": [2, 5], "tensor": [2, 7], "thi": [2, 3, 4, 5, 6, 7, 9], "batch": [2, 5], "return": [2, 3, 4, 5, 7], "correspond": 2, "reinforc": [2, 5, 9], "np": 2, "numpi": 2, "arrai": [2, 5], "each": [2, 4, 5, 9], "inform": [2, 3, 4, 9], "about": [2, 3, 4, 9], "singl": 2, "taken": [2, 4, 5, 9], "environ": [2, 3, 4, 5, 6, 9], "kei": [2, 7], "next_stat": [2, 5], "three": 2, "__get_act": [2, 3], "binomi": 2, "distribut": 2, "index": 2, "maximum": [2, 5, 9], "output": 2, "depend": 2, "certain": [2, 4], "condit": 2, "instanc": [2, 3, 4, 7, 9], "which": [2, 3, 4, 5, 7, 9], "typic": 2, "oper": [2, 3, 4], "posit": [2, 3, 4, 5], "includ": 2, "thing": 2, "like": 2, "": [2, 3, 4], "board": 2, "game": 2, "other": [2, 3, 4], "ani": [2, 3, 4], "need": 2, "relev": [2, 3, 4], "repres": [2, 3, 4, 5, 6, 9], "base": [2, 3, 4, 6, 9], "given": [2, 4, 5, 6, 9], "If": [2, 4, 6, 9], "set": [2, 4, 6], "gener": 2, "inventori": [2, 4], "number": [2, 4, 5, 6, 9], "trial": 2, "success": 2, "otherwis": [2, 4], "__update_target_net": 2, "load": [2, 4], "eval": 2, "put": 2, "evalu": 2, "where": [2, 5], "backpropag": 2, "through": 2, "time": [3, 4, 6], "integ": [3, 4, 6, 9], "result": 3, "divis": 3, "attribut": [3, 4, 7, 8, 9], "object": [3, 4, 5, 6, 7, 9], "data_path": 4, "data": [4, 6, 7], "n_period": [4, 6], "bool": [4, 6], "fals": 4, "string": 4, "path": 4, "directori": 4, "file": 4, "term": 4, "boolean": [4, 6], "indic": 4, "whether": [4, 6], "multi": 4, "current_episod": 4, "preprocessor": 4, "preprocess": 4, "list": [4, 5, 7], "historical_data": 4, "panda": [4, 6], "datafram": [4, 6], "histor": 4, "pd": 4, "over": 4, "__compute_reward": 4, "comput": 4, "price": [4, 6], "input": 4, "share": 4, "__initialize_st": 4, "__load_episod": 4, "df": [4, 6], "flag": 4, "being": 4, "pass": [4, 7], "argument": [4, 7], "howev": 4, "unnecessari": 4, "seem": 4, "__update_st": 4, "manag": 4, "amount": 4, "subtract": 4, "level": 4, "self": 4, "itself": 4, "should": [4, 6], "origin": 4, "reset": 4, "execut": [4, 10], "one": [4, 5], "within": 4, "rais": 4, "error": 4, "invalid": 4, "In": [4, 5], "case": [4, 5], "assum": 4, "make": [4, 9], "decis": [4, 9], "how": [4, 5], "much": 4, "item": 4, "obtain": 4, "after": [4, 5, 6], "swap": 4, "seri": [4, 6], "accordingli": 4, "10000": 5, "__make_room": 5, "randomli": 5, "delet": 5, "row": 5, "memori": [5, 9], "first": [5, 6, 7], "half": 5, "shift": 5, "remain": 5, "up": 5, "get_sampl": 5, "batch_siz": [5, 9], "specifi": [5, 6, 9], "select": 5, "buffer": [5, 9], "infer": 5, "mean": [5, 6], "dist2horizon": 5, "add": 5, "fix": 5, "usual": 5, "vector": 5, "describ": 5, "receiv": 5, "next": 5, "transit": 5, "refer": 5, "distanc": 5, "befor": [5, 6], "end": [5, 9], "keep": 5, "track": 5, "mani": 5, "ar": [5, 7], "left": 5, "when": 5, "qv": 6, "normalize_pric": 6, "fed": 6, "split": 6, "normal": 6, "trade": 6, "stand": 6, "variat": 6, "measur": 6, "volatil": 6, "ad": 6, "have": 6, "standard": 6, "deviat": 6, "substract": 6, "__call__": 6, "constructor": 6, "arg": [7, 8], "kwarg": [7, 9], "properti": [7, 8], "astensor": [7, 8], "convert": 7, "pytorch": 7, "call": 7, "final": 7, "cast": 7, "new": 7, "update_st": 7, "keyword": 7, "append": 8, "interact": 9, "provid": 9, "market": 9, "allow": 9, "retriev": 9, "pretrain": 9, "max_step": 9, "32": 9, "limit": 9, "all": 9, "begin": 9, "dure": 9, "exce": 9, "process": 9, "stop": 9, "code": 10, "avail": 10, "github": 10, "you": 10, "clone": 10, "repositori": 10, "follow": 10, "command": 10, "git": 10, "http": 10, "com": 10, "g0bel1n": 10, "cd": 10, "pip": 10, "r": 10, "requir": 10, "txt": 10}, "objects": {"ddql_optimal_execution.agent": [[2, 0, 1, "", "DDQL"], [3, 0, 1, "", "TWAP"]], "ddql_optimal_execution.agent.DDQL": [[2, 1, 1, "", "__complete_target"], [2, 1, 1, "", "__get_action"], [2, 1, 1, "", "__init__"], [2, 1, 1, "", "__update_target_net"], [2, 2, 1, "", "device"], [2, 1, 1, "", "eval"], [2, 2, 1, "", "gamma"], [2, 2, 1, "", "greediness"], [2, 2, 1, "", "greedy_decay_rate"], [2, 2, 1, "", "initial_greediness"], [2, 1, 1, "", "learn"], [2, 2, 1, "", "loss_fn"], [2, 2, 1, "", "lr"], [2, 2, 1, "", "main_net"], [2, 2, 1, "", "mode"], [2, 2, 1, "", "optimizer"], [2, 2, 1, "", "quadratic_penalty_coefficient"], [2, 2, 1, "", "state_size"], [2, 2, 1, "", "target_net"], [2, 2, 1, "", "target_update_rate"], [2, 1, 1, "", "train"]], "ddql_optimal_execution.agent.TWAP": [[3, 1, 1, "", "__get_action"], [3, 1, 1, "", "__init__"]], "ddql_optimal_execution.environnement": [[4, 0, 1, "", "MarketEnvironnement"]], "ddql_optimal_execution.environnement.MarketEnvironnement": [[4, 1, 1, "", "__compute_reward"], [4, 1, 1, "", "__init__"], [4, 1, 1, "", "__initialize_state"], [4, 1, 1, "", "__load_episode"], [4, 1, 1, "", "__update_state"], [4, 2, 1, "", "current_episode"], [4, 2, 1, "", "done"], [4, 1, 1, "", "get_state"], [4, 2, 1, "", "historical_data"], [4, 2, 1, "", "historical_data_series"], [4, 2, 1, "", "horizon"], [4, 2, 1, "", "initial_inventory"], [4, 2, 1, "", "multi_episodes"], [4, 2, 1, "", "n_periods"], [4, 2, 1, "", "preprocessor"], [4, 2, 1, "", "quadratic_penalty_coefficient"], [4, 1, 1, "", "reset"], [4, 2, 1, "", "state"], [4, 1, 1, "", "step"], [4, 1, 1, "", "swap_episode"]], "ddql_optimal_execution.experience_replay": [[5, 0, 1, "", "ExperienceReplay"]], "ddql_optimal_execution.experience_replay.ExperienceReplay": [[5, 1, 1, "", "__init__"], [5, 1, 1, "", "__make_room"], [5, 1, 1, "", "get_sample"], [5, 1, 1, "", "push"]], "ddql_optimal_execution.preprocessing": [[6, 0, 1, "", "Preprocessor"]], "ddql_optimal_execution.preprocessing.Preprocessor": [[6, 1, 1, "", "__call__"], [6, 1, 1, "", "__init__"], [6, 2, 1, "", "n_periods"]], "ddql_optimal_execution.state": [[7, 0, 1, "", "State"], [8, 0, 1, "", "StateArray"]], "ddql_optimal_execution.state.State": [[7, 1, 1, "", "__init__"], [7, 3, 1, "", "astensor"], [7, 1, 1, "", "copy"], [7, 1, 1, "", "update_state"]], "ddql_optimal_execution.state.StateArray": [[8, 1, 1, "", "__init__"], [8, 1, 1, "", "append"], [8, 3, 1, "", "astensor"]], "ddql_optimal_execution.trainer": [[9, 0, 1, "", "Trainer"]], "ddql_optimal_execution.trainer.Trainer": [[9, 1, 1, "", "__init__"], [9, 2, 1, "", "agent"], [9, 2, 1, "", "env"], [9, 2, 1, "", "exp_replay"], [9, 1, 1, "id0", "pretrain"], [9, 1, 1, "", "test"], [9, 1, 1, "id1", "train"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:attribute", "3": "py:property"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "attribute", "Python attribute"], "3": ["py", "property", "Python property"]}, "titleterms": {"welcom": 0, "ddql": [0, 2], "optim": 0, "execut": 0, "": 0, "document": 0, "content": 0, "main": 0, "object": 0, "quickstart": 1, "guid": 1, "ddql_optimal_execut": [2, 3, 4, 5, 6, 7, 8, 9], "agent": [2, 3], "twap": 3, "environn": 4, "marketenvironn": 4, "experience_replai": 5, "experiencereplai": 5, "preprocess": 6, "preprocessor": 6, "state": [7, 8], "statearrai": 8, "trainer": 9, "instal": 10}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.viewcode": 1, "sphinx.ext.intersphinx": 1, "sphinx": 57}, "alltitles": {"Welcome to DDQL Optimal Execution\u2019s documentation!": [[0, "welcome-to-ddql-optimal-execution-s-documentation"]], "Contents:": [[0, null]], "Main Objects": [[0, "main-objects"]], "Quickstart Guide": [[1, "quickstart-guide"]], "Installation": [[10, "installation"]], "ddql_optimal_execution.agent.DDQL": [[2, "ddql-optimal-execution-agent-ddql"]], "ddql_optimal_execution.agent.TWAP": [[3, "ddql-optimal-execution-agent-twap"]], "ddql_optimal_execution.environnement.MarketEnvironnement": [[4, "ddql-optimal-execution-environnement-marketenvironnement"]], "ddql_optimal_execution.experience_replay.ExperienceReplay": [[5, "ddql-optimal-execution-experience-replay-experiencereplay"]], "ddql_optimal_execution.preprocessing.Preprocessor": [[6, "ddql-optimal-execution-preprocessing-preprocessor"]], "ddql_optimal_execution.state.State": [[7, "ddql-optimal-execution-state-state"]], "ddql_optimal_execution.state.StateArray": [[8, "ddql-optimal-execution-state-statearray"]], "ddql_optimal_execution.trainer.Trainer": [[9, "ddql-optimal-execution-trainer-trainer"]]}, "indexentries": {"ddql (class in ddql_optimal_execution.agent)": [[2, "ddql_optimal_execution.agent.DDQL"]], "__complete_target() (ddql_optimal_execution.agent.ddql method)": [[2, "ddql_optimal_execution.agent.DDQL.__complete_target"]], "__get_action() (ddql_optimal_execution.agent.ddql method)": [[2, "ddql_optimal_execution.agent.DDQL.__get_action"]], "__init__() (ddql_optimal_execution.agent.ddql method)": [[2, "ddql_optimal_execution.agent.DDQL.__init__"]], "__update_target_net() (ddql_optimal_execution.agent.ddql method)": [[2, "ddql_optimal_execution.agent.DDQL.__update_target_net"]], "device (ddql_optimal_execution.agent.ddql attribute)": [[2, "ddql_optimal_execution.agent.DDQL.device"]], "eval() (ddql_optimal_execution.agent.ddql method)": [[2, "ddql_optimal_execution.agent.DDQL.eval"]], "gamma (ddql_optimal_execution.agent.ddql attribute)": [[2, "ddql_optimal_execution.agent.DDQL.gamma"]], "greediness (ddql_optimal_execution.agent.ddql attribute)": [[2, "ddql_optimal_execution.agent.DDQL.greediness"]], "greedy_decay_rate (ddql_optimal_execution.agent.ddql attribute)": [[2, "ddql_optimal_execution.agent.DDQL.greedy_decay_rate"]], "initial_greediness (ddql_optimal_execution.agent.ddql attribute)": [[2, "ddql_optimal_execution.agent.DDQL.initial_greediness"]], "learn() (ddql_optimal_execution.agent.ddql method)": [[2, "ddql_optimal_execution.agent.DDQL.learn"]], "loss_fn (ddql_optimal_execution.agent.ddql attribute)": [[2, "ddql_optimal_execution.agent.DDQL.loss_fn"]], "lr (ddql_optimal_execution.agent.ddql attribute)": [[2, "ddql_optimal_execution.agent.DDQL.lr"]], "main_net (ddql_optimal_execution.agent.ddql attribute)": [[2, "ddql_optimal_execution.agent.DDQL.main_net"]], "mode (ddql_optimal_execution.agent.ddql attribute)": [[2, "ddql_optimal_execution.agent.DDQL.mode"]], "optimizer (ddql_optimal_execution.agent.ddql attribute)": [[2, "ddql_optimal_execution.agent.DDQL.optimizer"]], "quadratic_penalty_coefficient (ddql_optimal_execution.agent.ddql attribute)": [[2, "ddql_optimal_execution.agent.DDQL.quadratic_penalty_coefficient"]], "state_size (ddql_optimal_execution.agent.ddql attribute)": [[2, "ddql_optimal_execution.agent.DDQL.state_size"]], "target_net (ddql_optimal_execution.agent.ddql attribute)": [[2, "ddql_optimal_execution.agent.DDQL.target_net"]], "target_update_rate (ddql_optimal_execution.agent.ddql attribute)": [[2, "ddql_optimal_execution.agent.DDQL.target_update_rate"]], "train() (ddql_optimal_execution.agent.ddql method)": [[2, "ddql_optimal_execution.agent.DDQL.train"]], "twap (class in ddql_optimal_execution.agent)": [[3, "ddql_optimal_execution.agent.TWAP"]], "__get_action() (ddql_optimal_execution.agent.twap method)": [[3, "ddql_optimal_execution.agent.TWAP.__get_action"]], "__init__() (ddql_optimal_execution.agent.twap method)": [[3, "ddql_optimal_execution.agent.TWAP.__init__"]], "marketenvironnement (class in ddql_optimal_execution.environnement)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement"]], "__compute_reward() (ddql_optimal_execution.environnement.marketenvironnement method)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.__compute_reward"]], "__init__() (ddql_optimal_execution.environnement.marketenvironnement method)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.__init__"]], "__initialize_state() (ddql_optimal_execution.environnement.marketenvironnement method)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.__initialize_state"]], "__load_episode() (ddql_optimal_execution.environnement.marketenvironnement method)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.__load_episode"]], "__update_state() (ddql_optimal_execution.environnement.marketenvironnement method)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.__update_state"]], "current_episode (ddql_optimal_execution.environnement.marketenvironnement attribute)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.current_episode"]], "done (ddql_optimal_execution.environnement.marketenvironnement attribute)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.done"]], "get_state() (ddql_optimal_execution.environnement.marketenvironnement method)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.get_state"]], "historical_data (ddql_optimal_execution.environnement.marketenvironnement attribute)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.historical_data"]], "historical_data_series (ddql_optimal_execution.environnement.marketenvironnement attribute)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.historical_data_series"]], "horizon (ddql_optimal_execution.environnement.marketenvironnement attribute)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.horizon"]], "initial_inventory (ddql_optimal_execution.environnement.marketenvironnement attribute)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.initial_inventory"]], "multi_episodes (ddql_optimal_execution.environnement.marketenvironnement attribute)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.multi_episodes"]], "n_periods (ddql_optimal_execution.environnement.marketenvironnement attribute)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.n_periods"]], "preprocessor (ddql_optimal_execution.environnement.marketenvironnement attribute)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.preprocessor"]], "quadratic_penalty_coefficient (ddql_optimal_execution.environnement.marketenvironnement attribute)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.quadratic_penalty_coefficient"]], "reset() (ddql_optimal_execution.environnement.marketenvironnement method)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.reset"]], "state (ddql_optimal_execution.environnement.marketenvironnement attribute)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.state"]], "step() (ddql_optimal_execution.environnement.marketenvironnement method)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.step"]], "swap_episode() (ddql_optimal_execution.environnement.marketenvironnement method)": [[4, "ddql_optimal_execution.environnement.MarketEnvironnement.swap_episode"]], "experiencereplay (class in ddql_optimal_execution.experience_replay)": [[5, "ddql_optimal_execution.experience_replay.ExperienceReplay"]], "__init__() (ddql_optimal_execution.experience_replay.experiencereplay method)": [[5, "ddql_optimal_execution.experience_replay.ExperienceReplay.__init__"]], "__make_room() (ddql_optimal_execution.experience_replay.experiencereplay method)": [[5, "ddql_optimal_execution.experience_replay.ExperienceReplay.__make_room"]], "get_sample() (ddql_optimal_execution.experience_replay.experiencereplay method)": [[5, "ddql_optimal_execution.experience_replay.ExperienceReplay.get_sample"]], "push() (ddql_optimal_execution.experience_replay.experiencereplay method)": [[5, "ddql_optimal_execution.experience_replay.ExperienceReplay.push"]], "preprocessor (class in ddql_optimal_execution.preprocessing)": [[6, "ddql_optimal_execution.preprocessing.Preprocessor"]], "__call__() (ddql_optimal_execution.preprocessing.preprocessor method)": [[6, "ddql_optimal_execution.preprocessing.Preprocessor.__call__"]], "__init__() (ddql_optimal_execution.preprocessing.preprocessor method)": [[6, "ddql_optimal_execution.preprocessing.Preprocessor.__init__"]], "n_periods (ddql_optimal_execution.preprocessing.preprocessor attribute)": [[6, "ddql_optimal_execution.preprocessing.Preprocessor.n_periods"]], "state (class in ddql_optimal_execution.state)": [[7, "ddql_optimal_execution.state.State"]], "__init__() (ddql_optimal_execution.state.state method)": [[7, "ddql_optimal_execution.state.State.__init__"]], "astensor (ddql_optimal_execution.state.state property)": [[7, "ddql_optimal_execution.state.State.astensor"]], "copy() (ddql_optimal_execution.state.state method)": [[7, "ddql_optimal_execution.state.State.copy"]], "update_state() (ddql_optimal_execution.state.state method)": [[7, "ddql_optimal_execution.state.State.update_state"]], "statearray (class in ddql_optimal_execution.state)": [[8, "ddql_optimal_execution.state.StateArray"]], "__init__() (ddql_optimal_execution.state.statearray method)": [[8, "ddql_optimal_execution.state.StateArray.__init__"]], "append() (ddql_optimal_execution.state.statearray method)": [[8, "ddql_optimal_execution.state.StateArray.append"]], "astensor (ddql_optimal_execution.state.statearray property)": [[8, "ddql_optimal_execution.state.StateArray.astensor"]], "trainer (class in ddql_optimal_execution.trainer)": [[9, "ddql_optimal_execution.trainer.Trainer"]], "__init__() (ddql_optimal_execution.trainer.trainer method)": [[9, "ddql_optimal_execution.trainer.Trainer.__init__"]], "agent (ddql_optimal_execution.trainer.trainer attribute)": [[9, "ddql_optimal_execution.trainer.Trainer.agent"]], "env (ddql_optimal_execution.trainer.trainer attribute)": [[9, "ddql_optimal_execution.trainer.Trainer.env"]], "exp_replay (ddql_optimal_execution.trainer.trainer attribute)": [[9, "ddql_optimal_execution.trainer.Trainer.exp_replay"]], "pretrain() (ddql_optimal_execution.trainer.trainer method)": [[9, "ddql_optimal_execution.trainer.Trainer.pretrain"], [9, "id0"]], "test() (ddql_optimal_execution.trainer.trainer method)": [[9, "ddql_optimal_execution.trainer.Trainer.test"]], "train() (ddql_optimal_execution.trainer.trainer method)": [[9, "ddql_optimal_execution.trainer.Trainer.train"], [9, "id1"]]}})